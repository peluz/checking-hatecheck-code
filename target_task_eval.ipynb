{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from data_utils import HateDataset, get_results\n",
    "from utils import initialize_seeds\n",
    "from randomized_testing import randomized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a94884",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b650d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/targetData.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98655830",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d718a5",
   "metadata": {},
   "source": [
    "## Before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ba76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"davidson2017\", \"founta2018\"]\n",
    "all_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5299563",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    model = BertForSequenceClassification.from_pretrained(f\"./hatecheck-experiments/Models/BERT_{name}_weighted/Final\")\n",
    "    trainer = Trainer(\n",
    "        model=model,         \n",
    "        args=TrainingArguments(\n",
    "            output_dir=(f\"./hatecheck-experiments/Models/BERT_{name}_weighted/test\"),\n",
    "            per_device_eval_batch_size = 64)\n",
    "    )\n",
    "    for dataset in data[2]:\n",
    "        print(f\"Evaluating model pretrained on {name} on {dataset} test set\")\n",
    "        _, preds = get_results(trainer, data[2][dataset])\n",
    "        all_preds[(name, dataset)] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12cf25",
   "metadata": {},
   "source": [
    "## After fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"random\", \"unseen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    for split in splits:\n",
    "        if split == \"random\":\n",
    "            model = BertForSequenceClassification.from_pretrained(f\"./hatecheck-experiments/Models/BERT_{name}_hatecheck_weighted/Final\")\n",
    "        else:\n",
    "            model = BertForSequenceClassification.from_pretrained(f\"./hatecheck-experiments/Models/BERT_{name}_hatecheck_weighted_unseen/Final\")\n",
    "        trainer = Trainer(\n",
    "            model=model,         \n",
    "            args=TrainingArguments(\n",
    "                output_dir=(f\"./hatecheck-experiments/Models/test\"),\n",
    "                per_device_eval_batch_size = 64)\n",
    "        )\n",
    "        for dataset in data[2]:\n",
    "            print(f\"Evaluating model pretrained on {name} on {dataset} test set and finetuned on hatecheck with {split} split\")\n",
    "            _, preds = get_results(trainer, data[2][dataset])\n",
    "            all_preds[(f\"{name}-{split}\", dataset)] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0d983",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"davidson\", \"founta\"]:\n",
    "    model = BertForSequenceClassification.from_pretrained(f\"./models/BERT_hateCheck+{name}_weighted/final\")\n",
    "    trainer = Trainer(\n",
    "        model=model,         \n",
    "        args=TrainingArguments(\n",
    "            output_dir=(f\"./hatecheck-experiments/Models/BERT_{name}_weighted/test\"),\n",
    "            per_device_eval_batch_size = 64)\n",
    "    )\n",
    "    for dataset in data[2]:\n",
    "        print(f\"Evaluating model pretrained on {name} on {dataset} test set\")\n",
    "        _, preds = get_results(trainer, data[2][dataset])\n",
    "        all_preds[(f\"hatecheck+{name}\", dataset)] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77703f74",
   "metadata": {},
   "source": [
    "## Models that have never seen target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "        if split == \"random\":\n",
    "            model = BertForSequenceClassification.from_pretrained(f\"./models/BERT_hateCheck_weighted/final\")\n",
    "        else:\n",
    "            model = BertForSequenceClassification.from_pretrained(f\"./models/BERT_hateCheck_weighted_unseen/final\")\n",
    "        trainer = Trainer(\n",
    "            model=model,         \n",
    "            args=TrainingArguments(\n",
    "                output_dir=(f\"./hatecheck-experiments/Models/test\"),\n",
    "                per_device_eval_batch_size = 64)\n",
    "        )\n",
    "        for dataset in data[2]:\n",
    "            print(f\"Evaluating model that has never seen target data on {dataset} test set and finetuned on hatecheck with {split} split\")\n",
    "            _, preds = get_results(trainer, data[2][dataset])\n",
    "            all_preds[(f\"BertOnly_{split}\", dataset)] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "davidson_labels = data[2][\"davidson2017\"].labels\n",
    "founta_labels = data[2][\"founta2018\"].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model1, model2, labels, trials):\n",
    "    preds1 = all_preds[model1]\n",
    "    preds2 = all_preds[model2]\n",
    "    randomized_test(preds1, preds2, labels, trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ede88",
   "metadata": {},
   "source": [
    "## Change after fine-tuning or mixed data trainin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017', 'davidson2017'), (\"davidson2017-random\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017', 'davidson2017'), (\"hatecheck+davidson\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017', 'founta2018'), (\"davidson2017-random\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017', 'founta2018'), (\"hatecheck+davidson\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75facaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('founta2018', 'founta2018'), (\"founta2018-random\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2cba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('founta2018', 'founta2018'), (\"hatecheck+founta\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f278e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('founta2018', 'davidson2017'), (\"founta2018-random\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('founta2018', 'davidson2017'), (\"hatecheck+founta\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf65574",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017-random', 'davidson2017'), (\"davidson2017\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('davidson2017-random', 'davidson2017'), (\"BertOnly_random\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa28091",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(('founta2018-random', 'founta2018'), (\"BertOnly_random\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fcbcd",
   "metadata": {},
   "source": [
    "## Fine-tuning vs mixed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22462b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare((\"davidson2017-random\", \"davidson2017\"), (\"hatecheck+davidson\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b1e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare((\"davidson2017-random\", \"founta2018\"), (\"hatecheck+davidson\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare((\"founta2018-random\", \"founta2018\"), (\"hatecheck+founta\", \"founta2018\"), founta_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare((\"founta2018-random\", \"davidson2017\"), (\"hatecheck+founta\", \"davidson2017\"), davidson_labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f089a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checking-hateCheck",
   "language": "python",
   "name": "checking-hatecheck"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
